{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac13265a-3f9a-4c6d-abcf-9ff1b78a5fdb",
   "metadata": {
    "id": "ac13265a-3f9a-4c6d-abcf-9ff1b78a5fdb"
   },
   "source": [
    "**PIPELINE:**\n",
    "\n",
    "In this notebook:\n",
    "1. Raw video -> labelled keypoints (YOLO)\n",
    "2. Labelled keypoints video -> directory of images (split into frames)\n",
    "3. Assign ID numbers to everything\n",
    "4. labelled keypoints (YOLO) -> csv\n",
    "5. labelled bounding boxes (YOLO) -> csv\n",
    "\n",
    "Not in this notebook:\n",
    "    4. generate training data table\n",
    "    5. [Manual labelling]\n",
    "    6. feature generation\n",
    "    7. punch classification model (RNN/LSTM)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e72b4-4abc-4c88-8cac-f37ed9076ca9",
   "metadata": {
    "id": "183e72b4-4abc-4c88-8cac-f37ed9076ca9"
   },
   "source": [
    "keypoints format:\n",
    "\n",
    "video id | sequence number | internal frame number(0 at seqstart) | target person | keypoints (as list)\n",
    "\n",
    "-> order keypoints by the left-to-right order of their appearance in the first frame of the sequence\n",
    "\n",
    "-> target person numbering resets per sequence\n",
    "\n",
    "labelling format (punch):\n",
    "\n",
    "video id | sequence number | start frame | end frame (=start+k) | target person | punch type (0-3) | include? (0-1)\n",
    "\n",
    "-> 0 = no punch; 1 = straight; 2 = hook; 3 = uppercut\n",
    "\n",
    "\n",
    "\n",
    "training data format:\n",
    "\n",
    "from labelling table, take rows with include==1 and access their keypoints by video id, start frame and target person\n",
    "\n",
    "video id | sequence number | start frame | end frame | target person | punch type | keypoints (as list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24a4eae-c970-4675-864c-75e87ada8134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "e24a4eae-c970-4675-864c-75e87ada8134",
    "outputId": "a4b816d6-c592-42a0-d195-8c6ef919f896"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saahil/miniconda3/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c78b512-8422-42fa-9f61-836c800d275d",
   "metadata": {
    "id": "1c78b512-8422-42fa-9f61-836c800d275d"
   },
   "outputs": [],
   "source": [
    "FRAMES_PER_SEQUENCE = 6\n",
    "OVERLAP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4af12a7-9a8a-4e6f-8ca5-361473051056",
   "metadata": {
    "id": "d4af12a7-9a8a-4e6f-8ca5-361473051056"
   },
   "outputs": [],
   "source": [
    "#Step 1: Extract keypoints from video\n",
    "def extractKeypoints(video_fp,outputsDirectory,model):\n",
    "    \"\"\"\n",
    "    Extract and return keypoints tensor from video_fp using YOLO 2D pose estimation model\n",
    "    \"\"\"\n",
    "\n",
    "    model = YOLO(f\"{model}-pose.pt\")\n",
    "    results = model(video_fp, project=output_dir, name=output_name, stream=False, save=True,max_det=5, save_conf=True, vid_stride=2, conf=0.4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befbafe7-4776-44c1-9bc1-7343398e6479",
   "metadata": {
    "id": "befbafe7-4776-44c1-9bc1-7343398e6479"
   },
   "outputs": [],
   "source": [
    "#Step 2: split labelled keypoints video into a folder of individual frames at dir frames_fp\n",
    "def splitVideoToFrames(labelledvideo_fp,frames_fp):\n",
    "    \"\"\"\n",
    "    Split video of labelled keypoints at labelledvideo_fp into individual frames\n",
    "    Save in directory frames_fp as jpg files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(frames_fp): \n",
    "        os.makedirs(frames_fp)\n",
    "    video = cv2.VideoCapture(labelledvideo_fp)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error opening video {labelledvideo_fp}\")\n",
    "    i = 1\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret: break\n",
    "        frame_filename = os.path.join(frames_fp, f\"frame_{i:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        i += 1\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0837abe0-5c66-47db-b670-b07e4dc7f3b3",
   "metadata": {
    "id": "0837abe0-5c66-47db-b670-b07e4dc7f3b3"
   },
   "outputs": [],
   "source": [
    "#Step 3: Assign person IDs to each character by sequence\n",
    "\n",
    "def assignIDNumbers(framesDir):\n",
    "    \"\"\"\n",
    "    Assign person IDs to each character in each sequence\n",
    "    Ordering:\n",
    "\n",
    "    Return value: dictionary of [1-indexed sequence number]-> dictionary of\n",
    "    [1-indexed character number]->[index in keypoints for each frame in sequence]\n",
    "\n",
    "    Assignment order:\n",
    "     - Left-to-right in first frame of sequence\n",
    "     - If a person doesn't appear in the first frame of a sequence don't classify him.\n",
    "     - One-indexed\n",
    "    \"\"\"\n",
    "\n",
    "    def modified_cossim(v1,v2):\n",
    "        \"\"\"\n",
    "        Helper function\n",
    "        Modified cosine similarity: only nonzero elements are considered\n",
    "        \"\"\"\n",
    "        v1p = np.where((v1 != 0) & (v2 != 0), v1, 0)\n",
    "        v2p = np.where((v1 != 0) & (v2 != 0), v2, 0)\n",
    "        if np.sum(v1p)==0 or np.sum(v2p)==0:\n",
    "            return 0\n",
    "        return (v1p @ v2p)/(np.linalg.norm(v1p)*np.linalg.norm(v2p))\n",
    "\n",
    "    def sortfn(x):\n",
    "        \"\"\"\n",
    "        Helper function\n",
    "        We will order people left-to-right by the minimum x coordinate of their nonzero\n",
    "        keypoints in the first frame of the sequence\n",
    "        \"\"\"\n",
    "        arr = np.array(sequence[0].keypoints.xy[x-1].tolist())[:,0]\n",
    "        if len(arr[arr>0])>0:\n",
    "            return arr[arr > 0].min()\n",
    "        return 0\n",
    "\n",
    "    def findCameraChanges(framesDir, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Helper function\n",
    "        Detect camera angle changes in a directory of frames. Return a list of zero-indexed frame numbers of such changes\n",
    "        \"\"\"\n",
    "        angle_change_frames = []\n",
    "        prev_hist = None\n",
    "\n",
    "        files = sorted(os.listdir(framesDir))\n",
    "\n",
    "        for file in files:\n",
    "            if not file.endswith('.jpg'): continue  # Skip non-JPG files\n",
    "\n",
    "            frame_path = os.path.join(framesDir, file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "\n",
    "            if frame is None: continue #skip unreadable frames\n",
    "\n",
    "            curr_hist = cv2.calcHist([frame], [0], None, [256], [0, 256])\n",
    "\n",
    "            if prev_hist is not None:\n",
    "                # Compare curr frame's histogram with prev frame's histogram\n",
    "                similarity = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n",
    "\n",
    "                if similarity < threshold:\n",
    "                    angle_change_frames.append(int(file[-8:-4])-1)\n",
    "\n",
    "            prev_hist = curr_hist\n",
    "\n",
    "        return angle_change_frames\n",
    "\n",
    "    ID_assignments = dict()\n",
    "    num_frames = len(results)\n",
    "\n",
    "    angleChangeFrames = findCameraChanges(framesDir)\n",
    "\n",
    "    for s in range(0,num_frames,OVERLAP):\n",
    "        #print(f\"Processing sequence {int(s/OVERLAP) + 1}\")\n",
    "        sequence = results[s:s+FRAMES_PER_SEQUENCE]\n",
    "        if len(sequence) != FRAMES_PER_SEQUENCE: break #means we're done processing\n",
    "\n",
    "        #ensure that the sequence doesn't contain any frames with camera angle changes\n",
    "        skip = False\n",
    "        for i in range(s,s+FRAMES_PER_SEQUENCE):\n",
    "            if i in angleChangeFrames:\n",
    "                skip = True\n",
    "                break\n",
    "        if skip: continue\n",
    "\n",
    "        #ensure that the sequence doesn't contain any frames without ID'd characters\n",
    "        skip = False\n",
    "        for i in range(FRAMES_PER_SEQUENCE):\n",
    "            if len(sequence[i].keypoints.xy[0])==0:\n",
    "                skip = True\n",
    "                break\n",
    "        if skip: continue\n",
    "\n",
    "        #characters dictionary\n",
    "        #character with key i will contain a list which is his tensor array indexes for each frame\n",
    "        characters = dict()\n",
    "\n",
    "        #name pts left to rights from first frame in seq\n",
    "        order = sorted([r+1 for r in range(len(sequence[0].keypoints.xy))], key =sortfn)\n",
    "        for r in range(len(sequence[0].keypoints.xy)): \n",
    "            characters[order[r]] = [r] \n",
    "\n",
    "        sims = [] #keep track of avg similarity at each new layer\n",
    "        bestsim = 0\n",
    "\n",
    "        #match indexes at curr \"layer\" to those from prev layer \"layer-1\"\n",
    "        for layer in range(1,FRAMES_PER_SEQUENCE):\n",
    "            indexes = [r for r in range(len(sequence[layer].keypoints.xy))]\n",
    "\n",
    "            #match every character in \"layer\" one at a time\n",
    "            for key in characters.keys():\n",
    "                #if character doesnt exist in prev layer then skip it\n",
    "                if characters[key][-1] >= len(sequence[layer-1].keypoints.xy): continue\n",
    "\n",
    "                #vector of character[key] in prev layer\n",
    "                v1 = np.array(sequence[layer-1].keypoints.xy[characters[key][-1]].tolist()).flatten()\n",
    "                bestindex = -1; maxsim = 0\n",
    "\n",
    "                #if we've used up all of the indexes just break\n",
    "                if not indexes:\n",
    "                    characters[key].append(-1)\n",
    "                    continue\n",
    "\n",
    "                #figure out the best index to match to character[key] in layer\n",
    "                for index in indexes:\n",
    "                    v2 = np.array(sequence[layer].keypoints.xy[index].tolist()).flatten()\n",
    "                    sim = modified_cossim(v1,v2)\n",
    "                    if sim >= maxsim:\n",
    "                        maxsim = sim\n",
    "                        bestindex = index\n",
    "\n",
    "                characters[key].append(bestindex)\n",
    "                if bestindex != -1: indexes.remove(bestindex)\n",
    "\n",
    "        characters_purged = dict()\n",
    "        for key in characters.keys():\n",
    "            if -1 not in characters[key]:\n",
    "                characters_purged[key] = characters[key]\n",
    "                \n",
    "        ID_assignments[int(s/OVERLAP)+1] = characters_purged #key: 1-indexed seq number, value: character assignments dict\n",
    "    return ID_assignments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e5e93f-402d-4186-bdd4-bb09e41ed364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Write keypoints to CSV\n",
    "def write_keypoints(keypoints_fp):\n",
    "    \"\"\"\n",
    "    Write keypoints data to CSV file with the following format\n",
    "     - Column 0: video ID\n",
    "     - Column 1: sequence number (1-indexed)\n",
    "     - Column 2: Internal frame number (0-indexed)\n",
    "     - Column 3: Person ID (1-indexed)\n",
    "     - Columns 4-37: keypoints (xyxy...xy)\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    for sequence in IDs.keys():\n",
    "        seqstart = (sequence-1)*OVERLAP\n",
    "        #print(f\"SEQUENCE: {sequence}\")\n",
    "        for internal_frame in range(FRAMES_PER_SEQUENCE): #frame within sequence\n",
    "            external_frame = seqstart + internal_frame #total frame\n",
    "            for person in IDs[sequence].keys():\n",
    "                if -1 not in IDs[sequence][person]:\n",
    "                    if len(IDs[sequence][person]) > internal_frame and IDs[sequence][person][internal_frame]!=-1:\n",
    "                        index = IDs[sequence][person][internal_frame]\n",
    "                        #print(f\"INDEX: {index}, PERSON: {person} FRAME: {internal_frame}\")\n",
    "                        keypoints = np.array(results[external_frame].keypoints.xy[index].tolist()).flatten().tolist()\n",
    "                        line = [output_name, sequence, internal_frame, person] + keypoints\n",
    "\n",
    "                        lines.append(line)\n",
    "    with open(keypoints_fp, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write all lines to the CSV file\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd67392-f25c-40de-9f2d-46f38322ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Write bounding boxes to CSV\n",
    "def write_boxes(boxes_fp):\n",
    "    \"\"\"\n",
    "    Write bounding box data to CSV with the following format\n",
    "     - Column 0: video ID\n",
    "     - Column 1: sequence number (1-indexed)\n",
    "     - Column 2: 1-indexed external frame number, 4-digit padded as string\n",
    "     - Column 3: person ID\n",
    "     - Columns 4-7: bounding boxes (xyxy)\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "    \n",
    "    for sequence in IDs.keys():\n",
    "        seqstart = (sequence-1)*OVERLAP\n",
    "        #print(f\"SEQUENCE: {sequence}\")\n",
    "        for internal_frame in range(FRAMES_PER_SEQUENCE): #frame within sequence\n",
    "            external_frame = seqstart + internal_frame #total frame\n",
    "            for person in IDs[sequence].keys():\n",
    "                if -1 not in IDs[sequence][person]:\n",
    "                    if len(IDs[sequence][person]) > internal_frame and IDs[sequence][person][internal_frame]!=-1:\n",
    "                        index = IDs[sequence][person][internal_frame]\n",
    "                        boxes = np.array(results[external_frame].boxes.xyxy[index].tolist()).flatten().tolist()\n",
    "                        line = [output_name,sequence,f\"{(external_frame+1):04d}\",person] + boxes\n",
    "    \n",
    "                        lines.append(line)\n",
    "    with open(boxes_fp, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write all lines to the CSV file\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b169ef-d59e-46ba-93db-38112c6059a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dir(fp):\n",
    "    \"\"\"\n",
    "    Delete directory at given filepath if it exists\n",
    "    \"\"\"\n",
    "    if os.path.exists(fp) and os.path.isdir(fp):\n",
    "        shutil.rmtree(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d96ecb3-0466-4a1f-ba9f-66abaf66b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvideos_dir = \"raw\"\n",
    "output_dir = \"labelled\"\n",
    "model = \"yolov8l\"\n",
    "\n",
    "def run_pipeline(video_fp):\n",
    "    \"\"\"\n",
    "    Run preprocessing pipeline on a given video from its filepath\n",
    "    Results are in output_dir/[video name]\n",
    "    \"\"\"\n",
    "\n",
    "    delete_dir(f\"{output_dir}/{output_name}\")\n",
    "    results = extractKeypoints(video_fp,output_dir,model)\n",
    "    splitVideoToFrames(labelledvideo_fp,frames_fp)\n",
    "    IDs = assignIDNumbers(frames_fp)\n",
    "    write_keypoints(keypoints_fp)\n",
    "    write_boxes(boxes_fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49357c-25aa-4c79-a3c5-79d7c52c1012",
   "metadata": {
    "id": "9d49357c-25aa-4c79-a3c5-79d7c52c1012"
   },
   "source": [
    "**INSTRUCTIONS FOR USE:**\n",
    "\n",
    "To run the preprocessing script, you need:\n",
    " - raw videos in file folder \"raw\", mp4 file format\n",
    " - created output directory \"labelled\"\n",
    "Note that file paths are all relative, so keep this notebook in the same directory that contains directories \"raw\" and \"labelled\"\n",
    "\n",
    "Additionally, the following libraries must be installed on your system: ultralytics, numpy, opencv-python (cv2)\n",
    "\n",
    "Then run the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8844986d-2ed3-4e56-bd3e-bb1cdc3fb512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "WARNING ⚠️ inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (1/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 380.6ms\n",
      "video 1/1 (2/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 361.2ms\n",
      "video 1/1 (3/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 399.0ms\n",
      "video 1/1 (4/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 408.4ms\n",
      "video 1/1 (5/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 395.0ms\n",
      "video 1/1 (6/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 389.4ms\n",
      "video 1/1 (7/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 388.3ms\n",
      "video 1/1 (8/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 425.3ms\n",
      "video 1/1 (9/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 402.4ms\n",
      "video 1/1 (10/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 397.7ms\n",
      "video 1/1 (11/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 400.5ms\n",
      "video 1/1 (12/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 400.6ms\n",
      "video 1/1 (13/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 408.7ms\n",
      "video 1/1 (14/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 408.8ms\n",
      "video 1/1 (15/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 427.5ms\n",
      "video 1/1 (16/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 538.5ms\n",
      "video 1/1 (17/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 469.8ms\n",
      "video 1/1 (18/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 448.8ms\n",
      "video 1/1 (19/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 401.1ms\n",
      "video 1/1 (20/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 401.7ms\n",
      "video 1/1 (21/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 392.9ms\n",
      "video 1/1 (22/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 398.4ms\n",
      "video 1/1 (23/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 396.1ms\n",
      "video 1/1 (24/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 389.8ms\n",
      "video 1/1 (25/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 383.9ms\n",
      "video 1/1 (26/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 379.7ms\n",
      "video 1/1 (27/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 382.9ms\n",
      "video 1/1 (28/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 395.9ms\n",
      "video 1/1 (29/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 524.0ms\n",
      "video 1/1 (30/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 413.2ms\n",
      "video 1/1 (31/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 437.5ms\n",
      "video 1/1 (32/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 429.9ms\n",
      "video 1/1 (33/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 418.0ms\n",
      "video 1/1 (34/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 409.8ms\n",
      "video 1/1 (35/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 433.2ms\n",
      "video 1/1 (36/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 404.5ms\n",
      "video 1/1 (37/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 407.1ms\n",
      "video 1/1 (38/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 405.5ms\n",
      "video 1/1 (39/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 413.3ms\n",
      "video 1/1 (40/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 400.8ms\n",
      "video 1/1 (41/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 397.7ms\n",
      "video 1/1 (42/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 410.4ms\n",
      "video 1/1 (43/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 508.1ms\n",
      "video 1/1 (44/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 397.8ms\n",
      "video 1/1 (45/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 406.5ms\n",
      "video 1/1 (46/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 433.5ms\n",
      "video 1/1 (47/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 407.3ms\n",
      "video 1/1 (48/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 435.6ms\n",
      "video 1/1 (49/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 463.7ms\n",
      "video 1/1 (50/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 420.5ms\n",
      "video 1/1 (51/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 420.2ms\n",
      "video 1/1 (52/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 435.5ms\n",
      "video 1/1 (53/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 451.6ms\n",
      "video 1/1 (54/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 408.3ms\n",
      "video 1/1 (55/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 412.1ms\n",
      "video 1/1 (56/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 438.5ms\n",
      "video 1/1 (57/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 408.1ms\n",
      "video 1/1 (58/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 415.9ms\n",
      "video 1/1 (59/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 405.3ms\n",
      "video 1/1 (60/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 431.4ms\n",
      "video 1/1 (61/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 407.0ms\n",
      "video 1/1 (62/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 397.9ms\n",
      "video 1/1 (63/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 447.7ms\n",
      "video 1/1 (64/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 445.9ms\n",
      "video 1/1 (65/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 434.8ms\n",
      "video 1/1 (66/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 441.6ms\n",
      "video 1/1 (67/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 421.2ms\n",
      "video 1/1 (68/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 445.0ms\n",
      "video 1/1 (69/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 467.4ms\n",
      "video 1/1 (70/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 398.4ms\n",
      "video 1/1 (71/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 426.0ms\n",
      "video 1/1 (72/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 415.0ms\n",
      "video 1/1 (73/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 418.0ms\n",
      "video 1/1 (74/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 450.3ms\n",
      "video 1/1 (75/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 438.5ms\n",
      "video 1/1 (76/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 464.0ms\n",
      "video 1/1 (77/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 407.9ms\n",
      "video 1/1 (78/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 444.2ms\n",
      "video 1/1 (79/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 422.8ms\n",
      "video 1/1 (80/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 412.1ms\n",
      "video 1/1 (81/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 405.7ms\n",
      "video 1/1 (82/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 436.6ms\n",
      "video 1/1 (83/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 424.8ms\n",
      "video 1/1 (84/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 396.0ms\n",
      "video 1/1 (85/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 411.5ms\n",
      "video 1/1 (86/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 467.8ms\n",
      "video 1/1 (87/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 406.0ms\n",
      "video 1/1 (88/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 411.0ms\n",
      "video 1/1 (89/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 397.8ms\n",
      "video 1/1 (90/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 425.8ms\n",
      "video 1/1 (91/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 413.2ms\n",
      "video 1/1 (92/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 399.3ms\n",
      "video 1/1 (93/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 403.6ms\n",
      "video 1/1 (94/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 428.8ms\n",
      "video 1/1 (95/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 408.6ms\n",
      "video 1/1 (96/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 403.0ms\n",
      "video 1/1 (97/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 406.2ms\n",
      "video 1/1 (98/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 430.0ms\n",
      "video 1/1 (99/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 410.3ms\n",
      "video 1/1 (100/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 413.6ms\n",
      "video 1/1 (101/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 424.1ms\n",
      "video 1/1 (102/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 431.0ms\n",
      "video 1/1 (103/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 4 persons, 418.7ms\n",
      "video 1/1 (104/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 405.4ms\n",
      "video 1/1 (105/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 411.6ms\n",
      "video 1/1 (106/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 466.7ms\n",
      "video 1/1 (107/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 415.0ms\n",
      "video 1/1 (108/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 5 persons, 405.6ms\n",
      "video 1/1 (109/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 422.7ms\n",
      "video 1/1 (110/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 1 person, 423.9ms\n",
      "video 1/1 (111/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 425.1ms\n",
      "video 1/1 (112/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 1 person, 402.9ms\n",
      "video 1/1 (113/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 1 person, 422.3ms\n",
      "video 1/1 (114/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 1 person, 430.9ms\n",
      "video 1/1 (115/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 1 person, 444.3ms\n",
      "video 1/1 (116/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 419.8ms\n",
      "video 1/1 (117/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 406.5ms\n",
      "video 1/1 (118/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 438.2ms\n",
      "video 1/1 (119/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 453.9ms\n",
      "video 1/1 (120/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 3 persons, 529.2ms\n",
      "video 1/1 (121/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 452.2ms\n",
      "video 1/1 (122/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 549.5ms\n",
      "video 1/1 (123/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 488.3ms\n",
      "video 1/1 (124/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 540.4ms\n",
      "video 1/1 (125/125) /Users/saahil/Desktop/stanford/cs229/project/raw/short10.mp4: 384x640 2 persons, 393.1ms\n",
      "Speed: 1.7ms preprocess, 423.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mlabelled/short10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Run through preprocessing pipeline for every video in rawvideos_dir\n",
    "rawvideos_dir = \"raw\"\n",
    "output_dir = \"labelled\"\n",
    "model = \"yolov8l\"\n",
    "\n",
    "\"\"\"\n",
    "    Run preprocessing pipeline on a given video from its filepath\n",
    "    Results are in output_dir/[video name]\n",
    "\"\"\"\n",
    "\n",
    "#videos_to_process = [f\"{rawvideos_dir}/{file}\" for file in os.listdir(rawvideos_dir)]\n",
    "videos_to_process = [\"raw/short10.mp4\"]\n",
    "for video_fp in  videos_to_process:\n",
    "    output_name = f\"{video_fp[len(rawvideos_dir)+1:][:-4]}\"\n",
    "    labelledvideo_fp = f\"{output_dir}/{output_name}/{video_fp[len(rawvideos_dir)+1:]}\"\n",
    "    frames_fp = f\"{output_dir}/{output_name}/frames\"\n",
    "    keypoints_fp = f\"{output_dir}/{output_name}/keypoints.csv\"\n",
    "    boxes_fp = f\"{output_dir}/{output_name}/boxes.csv\"\n",
    "\n",
    "    delete_dir(f\"{output_dir}/{output_name}\")\n",
    "    results = extractKeypoints(video_fp,output_dir,model)\n",
    "    splitVideoToFrames(labelledvideo_fp,frames_fp)\n",
    "    IDs = assignIDNumbers(frames_fp)\n",
    "    write_keypoints(keypoints_fp)\n",
    "    write_boxes(boxes_fp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834bff8a-7734-4529-9e56-565bde472867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
